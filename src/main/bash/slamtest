#! /bin/bash
# SlamTest, version $VERSION$.
# 
# For more information, run `slamtest -h`.

VERSION='$VERSION$'
AUTHOR='jcbrinfo <jcbrinfo@users.noreply.github.com>'
LICENSE='
       Copyright (c) 2016, jcbrinfo <jcbrinfo@users.noreply.github.com>.

       Permission to use, copy, modify, and/or distribute this software for
       any purpose with or without fee is hereby granted, provided that the
       above copyright notice and this permission notice appear in all copies.

       THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANT-
       IES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
       MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
       ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
       WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
       ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
       OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.'

# ##############################################################################
# Paths

# The following paths MUST NOT end with a slash.

readonly DEFAULT_TEST_SOURCE_DIRECTORY=src/test/resources
readonly DEFAULT_TEST_TARGET_DIRECTORY=target/test
test_source_directory="${DEFAULT_TEST_SOURCE_DIRECTORY}"
test_target_directory="${DEFAULT_TEST_TARGET_DIRECTORY}"

# ##############################################################################
# Exit statuses

readonly E_SUCCESS=0
readonly E_FAILURE=1
readonly E_USAGE=2
readonly E_MKDIR=3

# ##############################################################################
# Options

readonly SOURCE_DIRECTORY_OPTION=d
readonly FORMAT_OPTION=f
readonly TARGET_DIRECTORY_OPTION=g
readonly HELP_OPTION=h
readonly LOAD_MODULE_OPTION=l
readonly SKIP_INCOMPLETE_TESTS_OPTION=s
readonly TEST_CASE_OPTION=t
readonly VERSION_OPTION=v

# The option string for `getopts`.
readonly OPTION_STRING=":\
${SOURCE_DIRECTORY_OPTION}:${FORMAT_OPTION}:${TARGET_DIRECTORY_OPTION}:\
${HELP_OPTION}${LOAD_MODULE_OPTION}:${SKIP_INCOMPLETE_TESTS_OPTION}\
${TEST_CASE_OPTION}:${VERSION_OPTION}\
"

# ##############################################################################
# Formats

readonly VOID_FORMAT=_
readonly CSV_FORMAT=c
readonly JSON_FORMAT=j
readonly LONG_FORMAT=l
readonly SCORE_FORMAT=s
readonly MARKDOWN_TASK_LIST_FORMAT=t

# Default value of the `$FORMAT_OPTION` option.
readonly DEFAULT_FORMAT=tl

# ##############################################################################
# Global variables

# The tested command.
tested_command=

# The number of successes.
successes=

# The number of failures.
failures=

# The number of displayed results for a particular test case.
#
# Used internally for the JSON format. Whether or not this variable is set and
# used depends on the format of the result list.
test_case_results=

# Indicates if the test cases without an expected output file are skipped.
#
# `1` if true. `0` if false.
incomplete_tests_skipped=0

# ##############################################################################
# Test execution

##run_test in_path out_path [variant]
# Executes the specified test.
#
# @param in_path the path to the input file
# @param out_path the path to which the output is redirected
# @param variant the additional argument to pass to the tested program
run_test() {
	command -- "${tested_command[@]}" "${@:3}" < "$1" > "$2" 2>&1
}

##test_all
# Executes all the tests.
test_all() {
	local test_case

	for test_case in "${test_source_directory}"/in/*; do
		if [ -e "${test_case}" ]; then
			test_one_input "${test_case##*/}"
		fi
	done
}

##test_compare test_name out_dir_name [variant]
# Compares actual and expected outputs of the specified test.
#
# Expects the test has already been executed.
#
# @param test_name the filename of the test
# @param out_dir_name the filename of the output directories
# @param variant the last argument to pass to the tested program
test_compare() {
	local test_name="$1"
	local out_dir_name="$2"
	local variant=("${@:3}")
	local expected="${test_source_directory}/${out_dir_name}/${test_name}"
	local actual="${test_target_directory}/${out_dir_name}/${test_name}"
	local compare_program=
	local compare_options=()

	if [ -e "${actual}" ]; then
		# `cmp` is unable to compare anything else than regular files.
		if [ -f "${actual}" ] && [ -f "${expected}" ]; then
			compare_program=cmp
			compare_options=(-s)
		elif [ -f "${actual}" ] || [ -f "${expected}" ]; then
			# The behaviour of `diff` (see below) is unpredictable in this edge
			# case. So, we have to handle this case by ourselves.
			add_result OUT_MISMATCH \
					"unexpected output" "" "" "" \
					"${test_name}" "${out_dir_name}" "${variant[@]}"
			return
		else
			compare_program=diff
			compare_options=(-r)
		fi
		"${compare_program}" "${compare_options[@]}" -- \
				"${actual}" "${expected}" > /dev/null 2>&1
		case "$?" in
		0)
			add_result OK \
					"success" "" "" "" \
					"${test_name}" "${out_dir_name}" "${variant[@]}"
			;;
		1)
			add_result OUT_MISMATCH \
					"unexpected output" "" "" "" \
					"${test_name}" "${out_dir_name}" "${variant[@]}"
			;;
		*)
			add_result INTERNAL_ERROR \
					"internal error" "\`${compare_program}\` failed" "" "" \
					"${test_name}" "${out_dir_name}" "${variant[@]}"
			;;
		esac
	else
		add_result NO_ACTUAL_OUT \
				"actual output missing" "" "" "" \
				"${test_name}" "${out_dir_name}" "${variant[@]}"
	fi
}

##test_one test_name out_dir_name [variant]
# Executes the specified test and checks the result.
#
# @param test_name the filename of the test
# @param out_dir_name the filename of the output directories
# @param variant the last argument to pass to the tested program
test_one() {
	local test_name="$1"
	local out_dir_name="$2"
	local variant=("${@:3}")
	local expected_status_file="${test_source_directory}/status/${test_name}"
	local expected_status=0
	local actual_status

	if [ -f "${expected_status_file}" ]; then
		expected_status=$(< "${expected_status_file}")
	fi

	run_test "${test_source_directory}/in/${test_name}" \
			"${test_target_directory}/${out_dir_name}/${test_name}" \
			"${variant[@]}"
	actual_status="$?"
	if [ "${actual_status}" = "${expected_status}" ]; then
		test_compare "$@"
	else
		add_result STATUS_MISMATCH \
				"exit status not ${expected_status}: got ${actual_status}" "" \
				"${expected_status}" "${actual_status}" "${test_name}" \
				"${out_dir_name}" "${variant[@]}"
	fi
}

##test_one_input test_name
# Executes all the tests related to the same input file.
#
# @param test_name the name of the input file (the name of the test case)
test_one_input() {
	local test_name="$1"
	local expected_out_path
	local out_dir
	local out_dir_name

	echo_test_case_start "${test_name}"

	# Does this test case has at least one expected output?
	local has_out=0

	if [ -e "${test_source_directory}"/out/"${test_name}" ]; then
		has_out=1
		test_one "${test_name}" out
	fi
	for expected_out_path in "${test_source_directory}"/out-*/"${test_name}"; do
		if [ -e "${expected_out_path}" ]; then
			has_out=1
			out_dir="${expected_out_path%"/${test_name}"}"
			out_dir_name="${out_dir##*/}"
			test_one "${test_name}" "${out_dir_name}" "${out_dir_name#out-}"
		fi
	done
	if [ "${has_out}" = 0 ] && [ "${incomplete_tests_skipped}" = 0 ]; then
		add_result NO_EXPECTED_OUT "expected output missing" "" "" "" \
				"${test_name}" ""
	fi

	echo_test_case_end
}

# ##############################################################################
# Display

##add_result code description message expected_status actual_status test_name \
##		out_dir_name [variant]
# Adds a result.
#
# @param code the code to ouput for the CSV and JSON formats. The value `OK`
# must be used to describe a success.
# @param description a short description of the success or the failure
# @param message the additonal details. Empty except for internal errors.
# @param expected_status the expected exit status for this test. Empty if not
#		applicable.
# @param actual_status the actual exit status for this test. Empty if not
#		applicable.
# @param test_name the filename of the test
# @param out_dir_name the filename of the output directories. Empty if not
#		applicable.
# @param variant the last argument to pass to the tested program
add_result() {
	local code="$1"

	echo_result "$@"

	if [ "${code}" = OK ]; then
		successes=$((successes + 1))
	else
		failures=$((failures + 1))
	fi
}

##echo_summary total successes failures
# Writes the summary line.
#
# @param total the total number of tests
# @param successes the number of successes
# @param failures the number of failures

##echo_error message
# Writes the specified error message to the standard error output.
#
# @param message a short description of the error
echo_error() {
	printf "%s: %s\n" "$0" "$1" >&2
}

##echo_version
# Writes the version.
echo_version() {
	printf "%s\n" "${VERSION}"
}

##echo_result_list_start
# Displays the start of the result list.
#
# The actual implementation is chosen at run-time.

echo_result_list_start_csv() { :; }

echo_result_list_start_json() {
	printf '{'
}

echo_result_list_start_markdown_task_list() { :; }

##echo_test_case_start test_name
# Displays the start of a test case.
#
# The actual implementation is chosen at run-time.
#
# @param test_name the filename of the test case

echo_test_case_start_csv() { :; }

echo_test_case_start_json() {
	if [ $((successes + failures)) != 0 ]; then
		printf ','
	fi
	printf '\n	'
	string_to_json "${test_name}"
	printf ': {'
	test_case_results=0
}

echo_test_case_start_markdown_task_list() { :; }

##echo_result code description message expected_status actual_status test_name \
##		out_dir_name [variant]
# Displays a result.
#
# The actual implementation is chosen at run-time.
#
# @param code the code to ouput for the CSV and JSON formats. The value `OK`
# must be used to describe a success.
# @param description a short description of the success or the failure
# @param message the additional details. Empty except for internal errors.
# @param expected_status the expected exit status for this test. Empty if not
#		applicable.
# @param actual_status the actual exit status for this test. Empty if not
#		applicable.
# @param test_name the filename of the test
# @param out_dir_name the filename of the output directories. Empty if not
#		applicable.
# @param variant the last argument to pass to the tested program

echo_result_csv() {
	local code="$1"
	#local description="$2"
	local message="$3"
	local expected_status="$4"
	local actual_status="$5"
	local test_name="$6"
	local out_dir_name="$7"
	#local variant=("${@:8}")

	string_to_csv "${test_name}"
	printf ,
	string_to_csv "${out_dir_name}"
	printf ',%s,%s,%s,' "${code}" "${expected_status}" "${actual_status}"
	string_to_csv "${message}"
	printf '\r\n'
}

echo_result_json() {
	local code="$1"
	#local description="$2"
	local message="$3"
	local expected_status="${4:-null}"
	local actual_status="${5:-null}"
	#local test_name="$6"
	local out_dir_name="$7"
	#local variant=("${@:8}")

	if [ "${test_case_results}" != 0 ]; then
		printf ','
	fi
	printf '\n		'
	string_to_json "${out_dir_name}"
	printf ': {\n			"result": "%s",\n' "${code}"
	printf '			"expected_status": %s,\n' "${expected_status}"
	printf '			"actual_status": %s,\n' "${actual_status}"
	printf '			"message": '
	nullable_string_to_json "$message"
	printf '\n		}'
	test_case_results=$((test_case_results + 1))
}

echo_result_markdown_task_list() {
	local code="$1"
	local description="$2"
	local message="${3:+": $3"}"
	local expected_status="$4"
	local actual_status="$5"
	local test_name="$6"
	#local out_dir_name="$7"
	local variant=("${@:8}")

	if [ "${code}" = OK ]; then
		printf -- '- [x] '
	else
		printf -- '- [ ] '
	fi
	string_to_markdown_code "${test_name}"
	if [ "${#variant}" != 0 ]; then
		printf ' ('
		string_to_markdown_code "${variant}"
		printf ')'
	fi
	printf ': %s%s\n' "${description}" "${message}"
}

##echo_test_case_end
# Displays the end of a test case.
#
# The actual implementation is chosen at run-time.

echo_test_case_end_csv() { :; }

echo_test_case_end_json() {
	printf '\n	}'
}

echo_test_case_end_markdown_task_list() { :; }

##echo_result_list_end
# Displays the end of the result list.
#
# The actual implementation is chosen at run-time.

echo_result_list_end_csv() { :; }

echo_result_list_end_json() {
	printf '\n}\n'
}

echo_result_list_end_markdown_task_list() { :; }

##echo_before_summary
# Writes a empty line before the summary if needed.
#
# Must be called before writing the summary line. The actual implementation is
# chosen at run-time.

##echo_summary total successes failures
# Writes the summary line.
#
# The actual implementation is chosen at run-time.
#
# @param total the total number of tests
# @param successes the number of successes
# @param failures the number of failures

echo_summary_csv() {
	printf '%s,%s,%s\r\n' "$1" "$2" "$3"
}

echo_summary_json() {
	printf '{"tests": %s, "successes": %s, "failures": %s}\n' "$1" "$2" "$3"
}

echo_summary_long() {
	local total="$1"
	local successes="$2"
	local failures="$3"

	printf "%d " "${total}"
	if ((total >= 2)); then
		printf "tests"
	else
		printf "test"
	fi
	printf ", %d " "${successes}"
	if ((successes >= 2)); then
		printf "successes"
	else
		printf "success"
	fi
	printf ", %d " "${failures}"
	if ((failures >= 2)); then
		printf "failures"
	else
		printf "failure"
	fi
	echo .
}

echo_summary_score() {
	printf '%s/%s\n' "$2" "$1"
}

##set_format format
# Sets the output of this script to the specified format.
#
# Sets the following functions according to the specified format:
# * {@link #echo_result_list_start}
# * {@link #echo_test_case_start}
# * {@link #echo_result}
# * {@link #echo_test_case_end}
# * {@link #echo_result_list_end}
# * {@link #echo_before_summary}
# * {@link #echo_summary}
#
# @param format the format specification, as described in the “Display format
# 		specification” subsection of the built-in manual.
set_format() {
	local format="$1"

	if [ "${#format}" != 2 ]; then
		raise_invalid_argument "${FORMAT_OPTION}" "${format}"
	fi

	case "${format%?}" in
	"${VOID_FORMAT}")
		echo_result_list_start() { :; }
		echo_test_case_start() { :; }
		echo_result() { :; }
		echo_test_case_end() { :; }
		echo_result_list_end() { :; }
		;;
	"${CSV_FORMAT}")
		echo_result_list_start() { echo_result_list_start_csv "$@"; }
		echo_test_case_start() { echo_test_case_start_csv "$@"; }
		echo_result() { echo_result_csv "$@"; }
		echo_test_case_end() { echo_test_case_end_csv "$@"; }
		echo_result_list_end() { echo_result_list_end_csv "$@"; }
		;;
	"${JSON_FORMAT}")
		echo_result_list_start() { echo_result_list_start_json "$@"; }
		echo_test_case_start() { echo_test_case_start_json "$@"; }
		echo_result() { echo_result_json "$@"; }
		echo_test_case_end() { echo_test_case_end_json "$@"; }
		echo_result_list_end() { echo_result_list_end_json "$@"; }
		;;
	"${MARKDOWN_TASK_LIST_FORMAT}")
		echo_result_list_start() {
			echo_result_list_start_markdown_task_list "$@"
		}
		echo_test_case_start() { echo_test_case_start_markdown_task_list "$@"; }
		echo_result() { echo_result_markdown_task_list "$@"; }
		echo_test_case_end() { echo_test_case_end_markdown_task_list "$@"; }
		echo_result_list_end() { echo_result_list_end_markdown_task_list "$@"; }
		;;
	*)
		raise_invalid_argument "${FORMAT_OPTION}" "${format}"
		;;
	esac

	case "${format#?}" in
	"${VOID_FORMAT}")
		echo_summary() { :; }
		;;
	"${CSV_FORMAT}")
		echo_summary() { echo_summary_csv "$@"; }
		;;
	"${JSON_FORMAT}")
		echo_summary() { echo_summary_json "$@"; }
		;;
	"${LONG_FORMAT}")
		echo_summary() { echo_summary_long "$@"; }
		;;
	"${SCORE_FORMAT}")
		echo_summary() { echo_summary_score "$@"; }
		;;
	*)
		raise_invalid_argument "${FORMAT_OPTION}" "${format}"
		;;
	esac

	case "${format}" in
	?_ | _?)
		echo_before_summary() { :; }
		;;
	*)
		echo_before_summary() { echo; }
		;;
	esac
}

##nullable_string_to_json s
# Formats a nullable string as a JSON string or `null`.
#
# @param s the string to format. An empty string is converted to `null`.
nullable_string_to_json() {
	if [ -z "$1" ]; then
		printf 'null'
	else
		string_to_json "$1"
	fi
}

##string_to_csv s
# Formats a string as a CSV cell.
#
# @param s the string to format
string_to_csv() {
	printf '%s' "${1:+"\"${1//\"/\"\"}\""}"
}

##string_to_json s
# Formats a string as a JSON string.
#
# @param s the string to format
string_to_json() {
	declare -i i
	local c

	printf '"'
	for ((i=0; i < ${#1}; ++i)); do
		c="${1:$i:1}"
		case "${c}" in
		\\)
			printf '\\\\'
			;;
		\")
			printf '\\"'
			;;
		$'\b')
			printf '\\b'
			;;
		$'\f')
			printf '\\f'
			;;
		$'\n')
			printf '\\n'
			;;
		$'\r')
			printf '\\r'
			;;
		$'\t')
			printf '\\t'
			;;
		[[:cntrl:]])
			printf '\\u%04X' "'${c}"
			;;
		*)
			printf '%s' "${c}"
			;;
		esac
	done
	printf '"'
}

##string_to_markdown_code s
# Formats a string as a Markdown `<code>` element.
#
# @param s the string to format
string_to_markdown_code() {
	case "$1" in
	*[\`[:cntrl:]]*)
		printf '<code>'
		string_to_markdown "$1"
		printf '</code>'
		;;
	*)
		printf '`%s`' "$1"
		;;
	esac
}

##string_to_markdown s
# Formats a string as a XML text node written in Markdown.
#
# All C0 characters are converted into XML character/entity references.
# Does not escape properly strings at the start of a line.
#
# Note: Backticks are converted as XML character references because of a bug in
# the original Markdown parser. In fact, when you have two backslash-backtick
# pairs, the backslash looses its meaning. Here is an example:
#
# +-------------+-------------------------+
# | Markdown    | HTML                    |
# +-------------+-------------------------+
# | \`a         | <p>`a</p>               |
# | \`a\`       | <p>\<code>a\</code></p> |
# | &#96;a&#96; | <p>&#96;a&#96;</p>      |
# +-------------+-------------------------+
#
# @param s the string to format
string_to_markdown() {
	declare -i i
	local c

	for ((i=0; i < ${#1}; ++i)); do
		c="${1:$i:1}"
		case "${c}" in
		\&)
			printf '&amp;'
			;;
		\")
			printf '&quot;'
			;;
		\')
			printf '&apos;'
			;;
		\<)
			printf '&lt;'
			;;
		\>)
			printf '&gt;'
			;;
		[\`[:cntrl:]])
			printf '&#%d;' "'${c}"
			;;
		\\ | \* | _ | \[ | \])
			printf '\\%s' "${c}"
			;;
		*)
			printf '%s' "${c}"
			;;
		esac
	done
}

# ##############################################################################
# Built-in manual

##echo_help
# Writes the built-in manual.
echo_help() {
	echo "\
NAME
       slamtest — run tests

SYNOPSIS
       $0 \\
           [-${SOURCE_DIRECTORY_OPTION} <definition directory>] [-${FORMAT_OPTION} <format>] [-${TARGET_DIRECTORY_OPTION} <target directory>] \\
           [-${LOAD_MODULE_OPTION} <script’s path>] [-${SKIP_INCOMPLETE_TESTS_OPTION}] [-${TEST_CASE_OPTION} <test case name>] [tested_command…]
       $0 -${HELP_OPTION}
       $0 -${VERSION_OPTION}

DESCRIPTION
       SlamTest is a simple BASH script that runs a program against a series
       of inputs and checks that the outputs match the expected ones. This
       script should be invoked from the root directory of the project to
       test. For a explanation of how to describe the tests, see the “INPUT
       FILES” section. For the location of the actual outputs of the tested
       program, see “OUTPUT FILES”.

       The name of each test case reflects the name of the input file used for
       this test case.

OPTIONS
       -${SOURCE_DIRECTORY_OPTION} <definition directory>
              Set the path to the directory containing the test’s definitions.
              (Default: “${DEFAULT_TEST_SOURCE_DIRECTORY}”)

       -${FORMAT_OPTION} <format>
              Set the output format of the result list and of the summary line.
              For a description of the available choices, see the “Display
              format specification” subsection of the “EXTENDED DESCRIPTION”.
              (Default: “${DEFAULT_FORMAT}”)

       -${TARGET_DIRECTORY_OPTION} <target directory>
              Set the path to the directory containing the files generated du-
              ring the tests.
              (Default: “${DEFAULT_TEST_TARGET_DIRECTORY}”)

       -${HELP_OPTION}     Display the built-in manual (this manual).

       -${LOAD_MODULE_OPTION} <script’s path>
              Include (“source”) the specified BASH script. For more informa-
              tion on how to use this feature, see the “Redefining how to run
              a test” subsection of the “EXTENDED DESCRIPTION”.

       -${SKIP_INCOMPLETE_TESTS_OPTION}     Skip test cases without a file describing the expected output
              instead of counting them as failures.

       -${TEST_CASE_OPTION} <test case name>
              Instead of running all test cases found, run only the specified
              one.

              Note: A test case may contain more than one test. For details,
              see “INPUT FILE”.

       -${VERSION_OPTION}     Display the version information.

OPERANDS
       tested_command
              The command to test.

STDIN
       Not used.

INPUT FILES
       <definition directory>
              The directory containing the test’s definitions.
              (Default: “${DEFAULT_TEST_SOURCE_DIRECTORY}”)

       <definition directory>/in/<test case name>
              The input for the test case named “<test case name>“.

       <definition directory>/status/<test case name> (optional)
              The expected exit status for the test case named “<test case
              name>”. Contains only one line with the exit status in decimal,
              encoded in US-ASCII. If this file is absent, the expected exit
              status is 0.

       <definition directory>/out/<test case name>
              The expected output for the test case named “<test case name>”,
              when the tested command is invoked without any additional argu-
              ment.

       <definition directory>/out-<arg>/<test case name> (optional)
              The expected output for the test case named “<test case name>”,
              when the “<arg>” argument is appended to the tested command.

       For each test case found in “<definition directory>/in”, SlamTest looks
       for any file with the same name in “<definition directory>/out” and/or
       “<definition directory>/out-*”. For each output file found, a test is
       run using the found file as the expected output. When the expected
       output is located in a “<definition directory>/out-<arg>” directory,
       “<arg>” is added as an argument to the end of the tested command.

ENVIRONMENT VARIABLES
       None.

STDOUT
       Except when run using the “-h” or “-v” option, a list the results is
       written. This list is formatted as task list using the GitHub Flavored
       Markdown. Then, a summary line of the form
       “<t> tests, <s> successes, <f> failures.” is written.

STDERR
       The standard error output of the script is reserved for writing diag-
       nostic messages when SlamTest is incorrectly used.

OUTPUT FILES
       <target directory>
              The directory containing the files generated during the tests.
              (Default: “${DEFAULT_TEST_TARGET_DIRECTORY}”)

       <target directory>/out/<test case name>
              The actual output for the test case named “<test case name>”,
              when the tested command is invoked without any additional argu-
              ment. Compared against
              “<definition directory>/out/<test case name>”.

       <target directory>/out-<arg>/<test case name>
              The actual output for the test case named “<test case name>”,
              when the “<arg>” argument is appended to the tested command.
              Compared against
              “<definition directory>/out-<arg>/<test case name>”.

EXTENDED DESCRIPTION
   Redefining how to run a test
       By default, SlamTest runs the specified command by redirecting the
       standard input and outputs, and append an additional argument when
       needed (see “INPUT FILES”). However, there are some situation were the
       tested program need to be invoked differently. May be the input file
       are actually directories, may be the program requires the filenames as
       arguments… To handle these cases, SlamTest has the “-${LOAD_MODULE_OPTION}” that takes the
       path of a BASH script that defines the “run_test” function.

       The “run_test” is called to run each test and takes at most 3
       arguments, with the last one being optional. The arguments are,
       respectively:

       1. The path to the input file.

       2. The path to which the output is redirected.

       3. The additional argument to pass to the tested program (see “INPUT
          FILES”).

       The list of the arguments passed to SlamTest that are not options (that
       is the tested command if you do not redefine “run_test”) is stored as
       an array in the “tested_command” global variable.

       Here is the default definition of “run_test”:

              run_test() {
	              command -- \"\${tested_command[@]}\" \"\${@:3}\" \\
	                      < \"\$1\" > \"\$2\" 2>&1
              }


       Note: The script specified to the “-${LOAD_MODULE_OPTION}” is included (“sourced”) directly
       by SlamTest’s script. So, avoid defining any global variable (or BASH
       option) other than “run_test” in the included script in order to avoid
       disturbing the inner workings of main script.

   Display format specification
       Even if the default output of SlamTest is great, the “-${FORMAT_OPTION}” option allows
       you to customize this output. Its argument is composed of two charac-
       ter: one for the result list, then one for the summary line. The first
       character (for the list of results) can take the following values:

       ${VOID_FORMAT}      Write nothing at all.

       ${CSV_FORMAT}      Write the content of a RFC 4180 (CSV) file, without the header.
              For each test, write a row with the following 6 fields:

              1. The name of the test case.

              2. The filename of the directory of the expected output, or an
                 empty string if no such file was found. This field is empty
                 for test cases without expected output

              3. The degree of success of the test. For the list of the possi-
                 ble values, the “Result codes” subsection below.

              4. The expected exit status. This field is empty except if the
                 tested program returned an unexpected exit status.

              5. The exit status of the tested program. This field is empty
                 except if the tested program returned an unexpected exit sta-
                 tus.

              6. The additional details. This field is empty for most results
                 except for internal errors.

              Example:

                     foo,\"out\",OK,,,
                     bar,\"out-42\",STATUS_MISMATCH,0,21,
                     baz,,NO_EXPECTED_OUT,,,


       ${JSON_FORMAT}      Write a JSON object where the keys are the test cases’ names.
              Each value is another JSON object with the filename of the dir-
              ectory of the expected output as the key of each entry. For test
              cases without expected output, the JSON object contains an empty
              string as its one and only key. Each value is again a JSON ob-
              ject with the following keys:

              result
                     The degree of success of the test. For the list of the
                     possible values, the “Result codes” subsection below.

              expected_status
                     The expected exit status. Its value is null except if the
                     tested program returned an unexpected exit status.

              actual_status
                     The exit status of the tested program. Its value is null
                     except if the tested program returned an unexpected exit
                     status.

              message
                     The additional details. The corresponding value is null
                     for most results except internal errors.

              Example:

                     {
                         \"foo\": {
                             \"out\": {
                                 \"result\": \"OK\",
                                 \"expected_status\": null,
                                 \"actual_status\": null,
                                 \"message\": null
                             }
                         },
                         \"bar\": {
                             \"out-42\": {
                                 \"result\": \"STATUS_MISMATCH\",
                                 \"expected_status\": 0,
                                 \"actual_status\": 21,
                                 \"message\": null
                             }
                         },
                         \"baz\": {
                             \"\": {
                                 \"result\": \"NO_EXPECTED_OUT\",
                                 \"expected_status\": null,
                                 \"actual_status\": null,
                                 \"message\": null
                             }
                         }
                     }

       ${MARKDOWN_TASK_LIST_FORMAT}      Default: Write a GitHub task list.

              Example:

                     - [x] \`foo\`: success
                     - [ ] \`bar\` (\`42\`): exit status not 0: got 21
                     - [ ] \`baz\`: expected output missing

       The second character (for the summary line) can take the following
       values:

       ${VOID_FORMAT}      Write nothing at all.

       ${CSV_FORMAT}      Write the content of a RFC 4180 (CSV) file, without the header.
              Write a row with the following 3 fields:

              1. The total number of tests.

              2. The number of successes.

              3. The number of failures.

              Example:

                     3,1,2

       ${JSON_FORMAT}      Write a JSON object with the following keys:

              tests
                     The total number of tests.

              successes
                     The number of successes.

              failures
                     The number of failures.

              Example:

                     {\"tests\": 3, \"successes\": 1, \"failures\": 2}

       ${LONG_FORMAT}      Default: Enumerate the count of tests, successes and failures.

              Example:

                     3 tests, 1 success, 2 failures.

       ${SCORE_FORMAT}      Write a summary as a score, that is a fraction of the number of
              successes over the total number of tests.

              Example:

                     1/3

       Note:
              When none of the character of the format specification is “_”,
              an empty line divides the two outputs.

   Result codes
       Here are the strings that SlamTest uses to represent the result of a
       test in the CSV and JSON display formats:

       OK
              Success.

       STATUS_MISMATCH
              Unexpected exit status.

       OUT_MISMATCH
              Unexpected output.

       INTERNAL_ERROR
              Error raised by SlamTest itself (internal error). This usually
              means that the program used to compare files returned a unexpec-
              ted exit status.

       NO_ACTUAL_OUT
              The file containing the actual output of the test was not found.

       NO_EXPECTED_OUT
              No file describing the expected output found.

EXIT STATUS
       ${E_SUCCESS}      All tests pass.

       ${E_FAILURE}      At least one test fails.

       ${E_USAGE}      The script is not used correctly.

       ${E_MKDIR}      Unable to create the required directories.

EXAMPLES
       $0 java -cp target/main com.example.MyProgram
              Test the command “java -cp target/main com.example.MyProgram”,
              that is the usual command to run the Java class
              “target/main/com/example/MyProgram.class”.

AUTHOR
       ${AUTHOR}

LICENSING${LICENSE}

SEE ALSO
       comm, cmp, diff

       The “README.md” file of the SlamTest’s project.\
"
}

# ##############################################################################
# Errors

##raise_invalid_argument option_name argument
# Raise an error about a invalid option-argument.
#
# @param option_name the name of the option (without the dash)
# @param argument the specified argument
raise_invalid_argument() {
	raise_usage_error "illegal argument for the -$1 option: $2"
}

##raise_invalid_option option_name
# Raise an error about an invalid option.
#
# @param option_name the name of the option (without the dash)
raise_invalid_option() {
	raise_usage_error "illegal option: -$1"
}

##raise_missing_argument option_name
# Raise an error about a missing option-argument.
#
# @param option_name the name of the option (without the dash)
raise_missing_argument() {
	raise_usage_error "option -$1 requires an argument"
}

##raise_missing_command
# Raise an error because the command to test is missing.
raise_missing_command() {
	raise_usage_error "command missing"
}

##raise_usage_error message
# Raise an error concerning the arguments passed to the script.
#
# @param message a short description of the error
raise_usage_error() {
	echo_error "$1"
	printf "For more information, run “%s -%s”.\n" "$0" "${HELP_OPTION}"
	exit "${E_USAGE}"
}

# ##############################################################################
# Others

## create_directories directories…
# An alternative to `mkdir -p` that handles race conditions between multiple
# calls to `mkdir` on any POSIX system.
#
# @param directories the directories to create.
create_directories() {
	local directory
	local last_right_part
	local left_part
	local right_part

	for directory in "$@"; do
		{
			mkdir -p -- "${directory}" > /dev/null 2>&1
		} || {
			# `mkdir -p` failed, but it may be because of a race condition (or
			# a symbolic link). See <http://bit.ly/1VeuAUJ>.
			# We fallback by creating each component of the path one-by-one.
			# Note: If the path contains `../` or `./`, we just end up trying to
			# create the same directory multiple times.
			right_part="${directory}"
			left_part=
			while [ -n "${right_part}" ]; do
				last_right_part="${right_part}"
				right_part="${right_part#*/}"
				if [ "${last_right_part}" = "${right_part}" ]; then
					left_part="${left_part}${right_part}"
					right_part=
				else
					left_part="${left_part}${last_right_part%"$right_part"}"
				fi
				mkdir -- "${left_part}" > /dev/null 2>&1
			done
		}

		if [ \! -d "${directory}" ]; then
			# Directory creation really failed (example: creation denied).
			# Call `mkdir` again to know what exit status to return and to
			# display the correct error message.
			mkdir -- "${directory}"
			return
		fi
	done
}

##prepare_target
# Creates the directories for the generated files.
prepare_target() {
	if [ -e "${test_source_directory}/out" ]; then
		create_directories "${test_target_directory}/out" || return
	fi
	for out_dir in "${test_source_directory}"/out-*; do
		if [ -e "${out_dir}" ]; then
			create_directories "${test_target_directory}/${out_dir##*/}" \
					|| return
		fi
	done
}

##load_module script_path
# Loads (“sources”) the specified BASH script.
#
# @param script_path the path to the script
load_module() {
	. "$1"
}

##main arguments…
# The main routine.
#
# @param arguments the arguments passed to the program
main() {
	local total
	local option_name
	local module_loaded=0
	local test_case=

	set_format "${DEFAULT_FORMAT}"
	while getopts "${OPTION_STRING}" option_name; do
		case "${option_name}" in
		"${SOURCE_DIRECTORY_OPTION}")
			test_source_directory="${OPTARG%%/}"
			;;
		"${FORMAT_OPTION}")
			set_format "${OPTARG}"
			;;
		"${TARGET_DIRECTORY_OPTION}")
			test_target_directory="${OPTARG%%/}"
			;;
		"${HELP_OPTION}")
			echo_help
			return
			;;
		"${LOAD_MODULE_OPTION}")
			load_module "${OPTARG}"
			module_loaded=1
			;;
		"${SKIP_INCOMPLETE_TESTS_OPTION}")
			incomplete_tests_skipped=1
			;;
		"${TEST_CASE_OPTION}")
			test_case="${OPTARG}"
			;;
		"${VERSION_OPTION}")
			echo_version
			return
			;;
		\?)
			raise_invalid_option "${OPTARG}"
			;;
		:)
			raise_missing_argument "${OPTARG}"
			;;
		esac
	done
	shift $((OPTIND - 1))
	tested_command=("$@")

	if [ "${module_loaded}" = 0 ] && [ "${#tested_command}" = 0 ]; then
		raise_missing_command
	fi

	successes=0
	failures=0
	prepare_target || exit "${E_MKDIR}"
	echo_result_list_start
	if [ -z "${test_case}" ]; then
		test_all
	else
		test_one_input "${test_case}"
	fi
	echo_result_list_end

	total=$((successes + failures))
	echo_before_summary
	echo_summary "${total}" "${successes}" "${failures}"

	if ((failures > 0)); then
		exit "${E_FAILURE}"
	else
		exit "${E_SUCCESS}"
	fi
}

main "$@"
